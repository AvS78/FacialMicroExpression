{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import utils as np_utils\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features from the respective directories and label them to create dataset\n",
    "orig_dir=os.getcwd()\n",
    "orig_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Full HOF Data\n",
    "features_data_hof_full=[]\n",
    "features_label_hof_full=[]\n",
    "\n",
    "path=\"F:\\CA2b\\HOF_Full_Classified\"\n",
    "folders=os.listdir(path)\n",
    "t_count=0\n",
    "for f in folders:\n",
    "    count=1\n",
    "    folder_path=os.path.join(path, f)\n",
    "    files=os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        features=os.listdir(file_path)\n",
    "        for feature in features:\n",
    "            feature_path=os.path.join(file_path,feature)\n",
    "            histogram=np.load(feature_path)\n",
    "            histogram = np.mean(histogram, axis=0)\n",
    "            features_data_hof_full.append(histogram)\n",
    "            features_label_hof_full.append(f)\n",
    "            count=count+1\n",
    "            t_count=t_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 167796)\n"
     ]
    }
   ],
   "source": [
    "# Process HOF Type to correct format for DNN\n",
    "features_data_hof_full=np.reshape(features_data_hof_full, (167796, 255)).T\n",
    "print(features_data_hof_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Full LBP Data\n",
    "features_data_l=[]\n",
    "features_label_l=[]\n",
    "\n",
    "path=\"F:\\CA2b\\LBP_Top_Raw_Classified\"\n",
    "folders=os.listdir(path)\n",
    "t_count=0\n",
    "for f in folders:\n",
    "    count=1\n",
    "    folder_path=os.path.join(path, f)\n",
    "    \n",
    "    files=os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        \n",
    "        features=os.listdir(file_path)\n",
    "        for feature in features:\n",
    "            feature_path=os.path.join(file_path,feature)\n",
    "            histogram=np.load(feature_path)\n",
    "            histogram = np.mean(histogram, axis=0)\n",
    "            features_data_l.append(histogram)\n",
    "            features_label_l.append(f)\n",
    "            count=count+1\n",
    "            t_count=t_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 768)\n"
     ]
    }
   ],
   "source": [
    "# Process to correct format for DNN\n",
    "features_data_l=np.concatenate(features_data_l, axis=1).T\n",
    "features_data_l\n",
    "print(features_data_l.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve LBP_Eye data\n",
    "features_data_le=[]\n",
    "features_label_le=[]\n",
    "\n",
    "path=\"F:\\CA2b\\Eye_features_Classified_1\"\n",
    "folders=os.listdir(path)\n",
    "t_count=0\n",
    "for f in folders:\n",
    "    count=1\n",
    "    folder_path=os.path.join(path, f)\n",
    "    files=os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        features=os.listdir(file_path)\n",
    "        for feature in features:\n",
    "            feature_path=os.path.join(file_path,feature)\n",
    "            histogram=np.load(feature_path)\n",
    "            histogram = np.mean(histogram, axis=0)\n",
    "            features_data_le.append(histogram)\n",
    "            features_label_le.append(f)\n",
    "            count=count+1\n",
    "            t_count=t_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process to correct format for DNN\n",
    "features_data_le=np.concatenate(features_data_le, axis=1).T\n",
    "print(features_data_le.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve LBP_Mouth data\n",
    "features_data_lm=[]\n",
    "features_label_lm=[]\n",
    "path=r'F:\\CA2b\\Nose_Features_classified_1'\n",
    "folders=os.listdir(path)\n",
    "t_count=0\n",
    "for f in folders:\n",
    "    count=1\n",
    "    folder_path=os.path.join(path, f)\n",
    "    files=os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        features=os.listdir(file_path)\n",
    "        for feature in features:\n",
    "            feature_path=os.path.join(file_path,feature)\n",
    "            histogram=np.load(feature_path)\n",
    "            histogram = np.mean(histogram, axis=0)\n",
    "            features_data_lm.append(histogram)\n",
    "            features_label_lm.append(f)\n",
    "            count=count+1\n",
    "            t_count=t_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process to correct format for DNN\n",
    "features_data_lm=np.concatenate(features_data_lm, axis=1).T\n",
    "print(features_data_lm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve HOF data for eye and mouth and cropped\n",
    "cropped_hof_path='F:\\CA2b\\HOF_combined_dataset\\hof_feature_128x64, feature_hof.npy'\n",
    "eye_nose_hof_path='F:\\CA2b\\HOF_combined_dataset\\hof_feature_eye_nose, feature_hof.npy'\n",
    "other_label_path='F:\\CA2b\\HOF_combined_dataset\\label_hof_fullimage, feature_hof.npy'\n",
    "features_data_hof_eye_mouth=np.load(eye_nose_hof_path)\n",
    "features_data_hof_cropped=np.load(cropped_hof_path)\n",
    "label_other=np.load(other_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 4428)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine two types of features if needed\n",
    "X_Combined=np.concatenate((features_data_hof_cropped,features_data_hof_eye_mouth),axis=1)\n",
    "X_Combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X=features_data\n",
    "y=features_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 4428)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X=features_data\n",
    "X=X_Combined\n",
    "y=label_other\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder of labels\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "dummy_y = np_utils.to_categorical(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location='F:\\CA2b\\Run_time_data'\n",
    "modelname= 'RB_FF_NN13'\n",
    "savelocation=os.path.join(save_location, modelname)\n",
    "filepath=savelocation + \".hdf5\"\n",
    "csv_logger=CSVLogger(savelocation +'.csv')\n",
    "#callbacks_list  = [checkpoint,csv_logger,LRScheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 512)               2267648   \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 3,059,207\n",
      "Trainable params: 3,059,207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='sigmoid', input_shape=(4428,)))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[csv_logger, ModelCheckpoint(filepath,monitor='val_loss', save_best_only=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 202 samples, validate on 2 samples\n",
      "Epoch 1/200\n",
      "202/202 [==============================] - 10s 51ms/sample - loss: 1.8020 - accuracy: 0.3069 - val_loss: 1.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 1.6862 - accuracy: 0.3366 - val_loss: 1.6794 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 1.6755 - accuracy: 0.2871 - val_loss: 1.0818 - val_accuracy: 1.0000\n",
      "Epoch 4/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.6728 - accuracy: 0.3168 - val_loss: 0.7145 - val_accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.6817 - accuracy: 0.3416 - val_loss: 0.7520 - val_accuracy: 1.0000\n",
      "Epoch 6/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.6435 - accuracy: 0.3515 - val_loss: 0.6103 - val_accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.6853 - accuracy: 0.3317 - val_loss: 1.3100 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.6868 - accuracy: 0.2624 - val_loss: 1.1220 - val_accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.6648 - accuracy: 0.3119 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.6731 - accuracy: 0.3317 - val_loss: 0.7733 - val_accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.6705 - accuracy: 0.3317 - val_loss: 0.6414 - val_accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.7007 - accuracy: 0.3317 - val_loss: 1.0832 - val_accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.6678 - accuracy: 0.3119 - val_loss: 1.1766 - val_accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.6498 - accuracy: 0.3465 - val_loss: 0.7120 - val_accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.6651 - accuracy: 0.3564 - val_loss: 1.1939 - val_accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.6551 - accuracy: 0.3515 - val_loss: 0.9842 - val_accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.6422 - accuracy: 0.3416 - val_loss: 1.1204 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.6509 - accuracy: 0.3168 - val_loss: 1.1876 - val_accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.6218 - accuracy: 0.3515 - val_loss: 1.4548 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 1.6070 - accuracy: 0.3069 - val_loss: 1.0919 - val_accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.5998 - accuracy: 0.3614 - val_loss: 1.1946 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.5854 - accuracy: 0.3564 - val_loss: 0.8599 - val_accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.5831 - accuracy: 0.3515 - val_loss: 0.7611 - val_accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.5760 - accuracy: 0.3663 - val_loss: 0.8965 - val_accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 1.5673 - accuracy: 0.3762 - val_loss: 0.9492 - val_accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 1.5363 - accuracy: 0.3762 - val_loss: 0.8006 - val_accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.4504 - accuracy: 0.3960 - val_loss: 1.1911 - val_accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 1.3853 - accuracy: 0.3515 - val_loss: 1.0584 - val_accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.3823 - accuracy: 0.3564 - val_loss: 0.9236 - val_accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 1.2948 - accuracy: 0.3713 - val_loss: 0.8719 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.3348 - accuracy: 0.3911 - val_loss: 0.6716 - val_accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 1.3200 - accuracy: 0.4257 - val_loss: 0.8533 - val_accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.2585 - accuracy: 0.4208 - val_loss: 0.9326 - val_accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 1.2445 - accuracy: 0.4554 - val_loss: 0.7112 - val_accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.1794 - accuracy: 0.4950 - val_loss: 0.5451 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 1.1628 - accuracy: 0.4901 - val_loss: 0.7362 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 1.1135 - accuracy: 0.5545 - val_loss: 1.7261 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.0719 - accuracy: 0.5792 - val_loss: 0.6372 - val_accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.0729 - accuracy: 0.5693 - val_loss: 0.5805 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.0600 - accuracy: 0.5743 - val_loss: 0.5498 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 1.0667 - accuracy: 0.5990 - val_loss: 0.7019 - val_accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 1.0489 - accuracy: 0.5990 - val_loss: 0.3740 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.0103 - accuracy: 0.5990 - val_loss: 0.5269 - val_accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 1.0249 - accuracy: 0.6089 - val_loss: 0.3165 - val_accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 1.0192 - accuracy: 0.6040 - val_loss: 1.7573 - val_accuracy: 0.5000\n",
      "Epoch 46/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.0272 - accuracy: 0.6238 - val_loss: 0.3675 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.0352 - accuracy: 0.5792 - val_loss: 0.4294 - val_accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 1.0451 - accuracy: 0.5842 - val_loss: 0.3284 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.9474 - accuracy: 0.6139 - val_loss: 0.3878 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.9615 - accuracy: 0.6386 - val_loss: 0.4560 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 1.0275 - accuracy: 0.6089 - val_loss: 0.1864 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.9595 - accuracy: 0.6287 - val_loss: 0.2151 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.9802 - accuracy: 0.6535 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.9896 - accuracy: 0.6089 - val_loss: 0.2208 - val_accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.9407 - accuracy: 0.6386 - val_loss: 0.2163 - val_accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.9540 - accuracy: 0.6287 - val_loss: 0.3689 - val_accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.8720 - accuracy: 0.6287 - val_loss: 0.2192 - val_accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.8933 - accuracy: 0.6386 - val_loss: 0.2740 - val_accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.8267 - accuracy: 0.6634 - val_loss: 0.2511 - val_accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.8898 - accuracy: 0.6535 - val_loss: 0.1407 - val_accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.8561 - accuracy: 0.6733 - val_loss: 0.3113 - val_accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.8548 - accuracy: 0.6584 - val_loss: 0.1934 - val_accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.9750 - accuracy: 0.6535 - val_loss: 0.2592 - val_accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.8809 - accuracy: 0.6634 - val_loss: 0.8293 - val_accuracy: 0.5000\n",
      "Epoch 65/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.8575 - accuracy: 0.6832 - val_loss: 0.2757 - val_accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.8640 - accuracy: 0.6683 - val_loss: 0.5483 - val_accuracy: 0.5000\n",
      "Epoch 67/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.8489 - accuracy: 0.6832 - val_loss: 0.4298 - val_accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.7935 - accuracy: 0.6881 - val_loss: 0.8265 - val_accuracy: 0.5000\n",
      "Epoch 69/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.8412 - accuracy: 0.6782 - val_loss: 0.1075 - val_accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.8393 - accuracy: 0.7277 - val_loss: 0.4410 - val_accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.8212 - accuracy: 0.7079 - val_loss: 0.2891 - val_accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.7661 - accuracy: 0.7030 - val_loss: 0.1391 - val_accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.8222 - accuracy: 0.7475 - val_loss: 0.2000 - val_accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.7490 - accuracy: 0.7426 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.7611 - accuracy: 0.7277 - val_loss: 0.4927 - val_accuracy: 0.5000\n",
      "Epoch 76/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.7474 - accuracy: 0.7079 - val_loss: 0.2388 - val_accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.7057 - accuracy: 0.7525 - val_loss: 0.2358 - val_accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.7935 - accuracy: 0.7475 - val_loss: 0.1325 - val_accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.7255 - accuracy: 0.7277 - val_loss: 0.2168 - val_accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.7450 - accuracy: 0.7624 - val_loss: 0.4799 - val_accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.6445 - accuracy: 0.7624 - val_loss: 0.3167 - val_accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 0.7186 - accuracy: 0.7475 - val_loss: 0.6352 - val_accuracy: 0.5000\n",
      "Epoch 83/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.6880 - accuracy: 0.7327 - val_loss: 0.0916 - val_accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.6439 - accuracy: 0.7921 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.6209 - accuracy: 0.7871 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.7949 - accuracy: 0.7921 - val_loss: 0.5807 - val_accuracy: 0.5000\n",
      "Epoch 87/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.6411 - accuracy: 0.8168 - val_loss: 0.0674 - val_accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.6316 - accuracy: 0.7970 - val_loss: 0.4751 - val_accuracy: 0.5000\n",
      "Epoch 89/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.6833 - accuracy: 0.8020 - val_loss: 0.1504 - val_accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.5683 - accuracy: 0.8267 - val_loss: 0.0671 - val_accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.6557 - accuracy: 0.8218 - val_loss: 0.0979 - val_accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.6088 - accuracy: 0.8366 - val_loss: 0.0762 - val_accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.5540 - accuracy: 0.8465 - val_loss: 0.0888 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.7334 - accuracy: 0.8465 - val_loss: 0.1294 - val_accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.6538 - accuracy: 0.8119 - val_loss: 0.3384 - val_accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.5163 - accuracy: 0.9010 - val_loss: 0.1154 - val_accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.5872 - accuracy: 0.8564 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.7799 - accuracy: 0.8614 - val_loss: 0.2607 - val_accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.4370 - accuracy: 0.8663 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.5742 - accuracy: 0.8317 - val_loss: 0.1751 - val_accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.5533 - accuracy: 0.8069 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.5056 - accuracy: 0.8416 - val_loss: 0.1413 - val_accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.5580 - accuracy: 0.8465 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.4202 - accuracy: 0.8762 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.5576 - accuracy: 0.8564 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.4692 - accuracy: 0.8614 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4715 - accuracy: 0.8564 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.5039 - accuracy: 0.8812 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.4262 - accuracy: 0.8663 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.5463 - accuracy: 0.8515 - val_loss: 0.2064 - val_accuracy: 1.0000\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.4201 - accuracy: 0.8713 - val_loss: 0.0879 - val_accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.5778 - accuracy: 0.8416 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4384 - accuracy: 0.8812 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.5107 - accuracy: 0.8465 - val_loss: 0.9417 - val_accuracy: 0.5000\n",
      "Epoch 115/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.6313 - accuracy: 0.8812 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3769 - accuracy: 0.8812 - val_loss: 0.0661 - val_accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4209 - accuracy: 0.8812 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.4672 - accuracy: 0.8812 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.4019 - accuracy: 0.8762 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3892 - accuracy: 0.8911 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.4385 - accuracy: 0.8911 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.5241 - accuracy: 0.8762 - val_loss: 1.1360 - val_accuracy: 0.5000\n",
      "Epoch 123/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3855 - accuracy: 0.9010 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.5181 - accuracy: 0.8416 - val_loss: 0.7560 - val_accuracy: 0.5000\n",
      "Epoch 125/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.4792 - accuracy: 0.8218 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3588 - accuracy: 0.8861 - val_loss: 1.0007 - val_accuracy: 0.5000\n",
      "Epoch 127/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3505 - accuracy: 0.8960 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 0.4257 - accuracy: 0.8960 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.3263 - accuracy: 0.9059 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 0.4224 - accuracy: 0.9109 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.4232 - accuracy: 0.8861 - val_loss: 0.0230 - val_accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4024 - accuracy: 0.8911 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4694 - accuracy: 0.8861 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.4804 - accuracy: 0.8812 - val_loss: 0.0739 - val_accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3693 - accuracy: 0.9010 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.3562 - accuracy: 0.8911 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.3549 - accuracy: 0.8861 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.3975 - accuracy: 0.8911 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.4139 - accuracy: 0.8861 - val_loss: 0.0937 - val_accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.3147 - accuracy: 0.9208 - val_loss: 0.1346 - val_accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.3790 - accuracy: 0.9109 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.4021 - accuracy: 0.9059 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.3666 - accuracy: 0.9208 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3032 - accuracy: 0.9158 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.2617 - accuracy: 0.9307 - val_loss: 8.2722e-04 - val_accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.4571 - accuracy: 0.8812 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.3441 - accuracy: 0.9208 - val_loss: 3.2633e-04 - val_accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.3388 - accuracy: 0.9208 - val_loss: 4.9957e-04 - val_accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.3962 - accuracy: 0.9109 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.3053 - accuracy: 0.9059 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3490 - accuracy: 0.9059 - val_loss: 5.2547e-04 - val_accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 0.4626 - accuracy: 0.9257 - val_loss: 0.3789 - val_accuracy: 0.5000\n",
      "Epoch 153/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.3537 - accuracy: 0.9208 - val_loss: 4.0569e-04 - val_accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.4428 - accuracy: 0.9059 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.3355 - accuracy: 0.8911 - val_loss: 1.5436e-04 - val_accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.3127 - accuracy: 0.9208 - val_loss: 4.0649e-04 - val_accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "202/202 [==============================] - 9s 44ms/sample - loss: 0.2876 - accuracy: 0.9356 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.1912 - accuracy: 0.9406 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.7276 - accuracy: 0.7624 - val_loss: 2.1991 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.4306 - accuracy: 0.8515 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.3387 - accuracy: 0.9059 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.2378 - accuracy: 0.9208 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.4342 - accuracy: 0.9010 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.7458 - accuracy: 0.8267 - val_loss: 0.1827 - val_accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.3229 - accuracy: 0.9257 - val_loss: 0.0067 - val_accuracy: 1.0000\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4093 - accuracy: 0.8663 - val_loss: 0.1609 - val_accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.5670 - accuracy: 0.9059 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.2967 - accuracy: 0.9158 - val_loss: 3.1447e-04 - val_accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.2637 - accuracy: 0.9158 - val_loss: 2.4469e-04 - val_accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.2437 - accuracy: 0.9505 - val_loss: 4.6311e-05 - val_accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.7695 - accuracy: 0.8812 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.3125 - accuracy: 0.9158 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.6434 - accuracy: 0.9406 - val_loss: 7.7570e-04 - val_accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.2579 - accuracy: 0.9356 - val_loss: 1.5394e-04 - val_accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.2327 - accuracy: 0.9356 - val_loss: 1.3769e-05 - val_accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4492 - accuracy: 0.9010 - val_loss: 0.1534 - val_accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.2484 - accuracy: 0.9158 - val_loss: 0.5234 - val_accuracy: 0.5000\n",
      "Epoch 178/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.3499 - accuracy: 0.9356 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.4241 - accuracy: 0.9158 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.1864 - accuracy: 0.9406 - val_loss: 1.2569e-04 - val_accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "202/202 [==============================] - 10s 47ms/sample - loss: 0.2798 - accuracy: 0.9257 - val_loss: 4.7684e-06 - val_accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.4539 - accuracy: 0.9406 - val_loss: 8.7493e-05 - val_accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.2897 - accuracy: 0.9307 - val_loss: 2.8669e-05 - val_accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.2302 - accuracy: 0.9356 - val_loss: 1.5453e-04 - val_accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.2229 - accuracy: 0.9554 - val_loss: 2.8729e-05 - val_accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.4086 - accuracy: 0.9257 - val_loss: 1.0281e-04 - val_accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.4370 - accuracy: 0.9109 - val_loss: 5.4319e-04 - val_accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.2124 - accuracy: 0.9455 - val_loss: 0.7685 - val_accuracy: 0.5000\n",
      "Epoch 189/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.3079 - accuracy: 0.9257 - val_loss: 5.6753e-04 - val_accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.2812 - accuracy: 0.9356 - val_loss: 7.0333e-06 - val_accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "202/202 [==============================] - 9s 47ms/sample - loss: 0.2859 - accuracy: 0.9406 - val_loss: 0.2083 - val_accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.3451 - accuracy: 0.9307 - val_loss: 8.8810e-06 - val_accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.2290 - accuracy: 0.9505 - val_loss: 8.8923e-05 - val_accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.2409 - accuracy: 0.9307 - val_loss: 6.7350e-05 - val_accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.3790 - accuracy: 0.9208 - val_loss: 1.5557e-05 - val_accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.1391 - accuracy: 0.9554 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "202/202 [==============================] - 10s 48ms/sample - loss: 0.2810 - accuracy: 0.9554 - val_loss: 4.8278e-05 - val_accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "202/202 [==============================] - 10s 49ms/sample - loss: 0.3143 - accuracy: 0.9554 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "202/202 [==============================] - 9s 46ms/sample - loss: 0.2621 - accuracy: 0.9307 - val_loss: 1.7467e-04 - val_accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "202/202 [==============================] - 9s 45ms/sample - loss: 0.1973 - accuracy: 0.9406 - val_loss: 1.9034e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "opt='RMSProp'\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size = 0.20)\n",
    "history = model.fit(X_train, y_train, batch_size=2, epochs=200, verbose=1,validation_split=0.005, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0  0  0  0  1]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [ 2  0  8  0  1  0  0]\n",
      " [ 0  0  0 17  1  0  2]\n",
      " [ 0  0  0  0  3  0  0]\n",
      " [ 0  0  0  0  0  1  1]\n",
      " [ 0  0  0  0  1  0  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83        11\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      0.73      0.84        11\n",
      "           3       1.00      0.85      0.92        20\n",
      "           4       0.50      1.00      0.67         3\n",
      "           5       1.00      0.50      0.67         2\n",
      "           6       0.33      0.67      0.44         3\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.66      0.66      0.62        51\n",
      "weighted avg       0.86      0.80      0.81        51\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\classifier6\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "y_test_original = y_test.argmax(1)\n",
    "y_train_original = y_train.argmax(1)\n",
    "labels = np.unique(y_test_original)\n",
    "a= confusion_matrix(y_test_original, yhat_classes)\n",
    "print(a)\n",
    "print(classification_report(y_test_original, yhat_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
